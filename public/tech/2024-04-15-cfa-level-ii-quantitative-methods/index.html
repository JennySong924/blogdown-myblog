<!DOCTYPE html>
<html lang="en-US">

  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=4321&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>CFA Level II - Quantitative methods - J Song&#39;s Blog</title>
    <meta property="og:title" content="CFA Level II - Quantitative methods - J Song&#39;s Blog">
    
    <meta name="twitter:card" content="summary">
    
      
      <meta property="description" content="The regression process covers several decisions the analyst must make, such as identifying the dependent and independent variables, selecting the appropriate regression model, testing if the &amp;hellip;">
      <meta property="og:description" content="The regression process covers several decisions the analyst must make, such as identifying the dependent and independent variables, selecting the appropriate regression model, testing if the &amp;hellip;">
      
    

    
    
    

    

    

    
    <link rel="stylesheet" href="/css/style.css">
    
    <link rel="stylesheet" href="/css/fonts.css">
    
    <link rel="stylesheet" href="/css/custom.css">
    
    






  </head>

  
  
  
  <body class="single tech">
    <div class="crop-h"></div><div class="crop-v"></div><div class="crop-c"></div>
    <nav class="nav-top small">
    <div class="logo">
    
    </div>
    <div class="menu"><span><a href="/search">ğŸ”search</a></span>
      <span><a href="/">Home</a></span>
      <span><a href="/post/">Post</a></span>
      <span class="active"><a href="/tech/">Tech</a></span>
      <span><a href="/reading/">Reading</a></span>
      <span><a href="/lists/">Lists</a></span>
      <span><a href="/about/">About</a></span>
      <span><a href="https://github.com/JennySong924/blogdown-myblog">GitHub</a></span>
      
    </div>
    </nav>

<div class="article-meta">
<h1 class="title">CFA Level II - Quantitative methods</h1>

<h3 class="meta-line">
  <span>

<span class="author">J Song</span>






<span class="date">2024-04-15</span>


</span>
  <span class="term">
  
  
  </span>
</h3>
</div>

<div class="main">





<nav id="TableOfContents">
      <ul>
        <li><a href="#learning-module-1-basics-of-multiple-regression-and-underlying-assumptions">Learning Module 1: Basics of multiple regression and underlying assumptions</a></li>
        <li><a href="#learning-module-2-evaluating-regression-model-fit-and-interpreting-model-results">Learning Module 2: Evaluating regression model fit and interpreting model results</a></li>
        <li><a href="#learning-module-3-model-misspecification">Learning Module 3: Model misspecification</a></li>
        <li><a href="#learning-module-4-extensions-of-multiple-regression">Learning Module 4: Extensions of multiple regression</a></li>
        <li><a href="#learning-module-5-time-series-analysis">Learning Module 5: Time-series analysis</a></li>
        <li><a href="#learning-module-6-machine-learning">Learning Module 6: Machine learning</a></li>
        <li><a href="#learning-module-7-big-data-projects">Learning Module 7: Big data projects</a></li>
      </ul>
</nav>




<h3 id="learning-module-1-basics-of-multiple-regression-and-underlying-assumptions">Learning Module 1: Basics of multiple regression and underlying assumptions</h3>
<ol>
<li>Uses of multiple linear regression
<ul>
<li>Multiple linear regression is used to model the linear relationship between one dependent variable and two or more independent variables.</li>
<li>In practice, multiple regressions are used to explain relationships between financial variables, to test existing theories, or to make forecasts.</li>
</ul>
</li>
<li>The basics of multiple regression
<ul>
<li>
<p>The regression process covers several decisions the analyst must make, such as identifying the dependent and independent variables, selecting the appropriate regression model, testing if the assumptions behind linear regression are satisfied, examining goodness of fit, and making needed adjustments.</p>
</li>
<li>
<p>The regression process</p>
<p><img src="1%20Quantitative%20Methods%20e84dd0d2cb6140a5a200ff6cf9e94216/quantitative-methods.drawio-3.png" alt="quantitative-methods.drawio-3.png"></p>
</li>
<li>
<p>A multiple regression model is represented by the following equation:
$Y_i = b_0 + b_1X_{1i} + b_2X_{2i} + \dots + b_kX_{ki} + \epsilon_i, i=1,2,3,&hellip;,n$
where $Y$ is the dependent variable, $Xs$ are the independent variables from $1$  to $k$, and the model is estimated using $n$ observations</p>
</li>
<li>
<p>Coefficient  $b_0$ is the modelâ€™s â€œinterceptâ€ representing the expected value of  $<em>Y$</em> if all independent variables are zero.</p>
</li>
<li>
<p>Parameters $<em>b_1</em>$ to $<em>b_k$</em> are the slope coefficients (or partial regression coefficients) for independent variables $<em>X_1</em>$ to $<em>X_k</em>$. Slope coefficient $<em>b_j</em>$ describes the impact of independent variable $<em>X_j$</em> on $<em>Y</em>$, holding all the other independent variables constant.</p>
</li>
</ul>
</li>
<li>Assumptions underlying multiple linear regression
<table>
<thead>
<tr>
<th>Assumptions</th>
<th>Description</th>
<th>Diagnose</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linearity</td>
<td>The relationship between the dependent variable and the independent variables is linear.</td>
<td>Scatterplots of dependent versus and independent variables (pairs plot) â€” also identify extreme values and outliers</td>
</tr>
<tr>
<td>Homoskedasticity</td>
<td>The variance of the regression residuals is the same for all observations.</td>
<td>Scatter plot of residuals against the dependent variable</td>
</tr>
<tr>
<td>Independence of errors</td>
<td>The observations are independent of one another. This implies the regression residuals are uncorrelated across observations.</td>
<td>Scatter plot of residuals against the dependent variable</td>
</tr>
<tr>
<td>Normality</td>
<td>The regression residuals are normally distributed.</td>
<td>Normal Q-Q plot</td>
</tr>
<tr>
<td>Independence of independent variables</td>
<td>1. Independent variables are not random.</td>
<td></td>
</tr>
<tr>
<td>2. There is no exact linear relation between two or more of the independent variables or combinations of the independent variables.</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<ul>
<li>
<p>Fat-tailed Q-Q plot</p>
<p><img src="1%20Quantitative%20Methods%20e84dd0d2cb6140a5a200ff6cf9e94216/Screenshot_2024-04-07_at_13.18.05.png" alt="Screenshot 2024-04-07 at 13.18.05.png"></p>
</li>
</ul>
</li>
</ol>
<h3 id="learning-module-2-evaluating-regression-model-fit-and-interpreting-model-results">Learning Module 2: Evaluating regression model fit and interpreting model results</h3>
<ol>
<li>Analysis of variance, ANOVA æ–¹å·®åˆ†æ</li>
</ol>
<table>
<thead>
<tr>
<th></th>
<th>degrees of freedom, df</th>
<th>sum of squares, SS</th>
<th>mean sum of squares, MS</th>
</tr>
</thead>
<tbody>
<tr>
<td>Regression</td>
<td>k</td>
<td>SSR</td>
<td>MSR = SSR/k</td>
</tr>
<tr>
<td>Error</td>
<td>n-k-1</td>
<td>SSE</td>
<td>MSE = SSE/(n-k-1)</td>
</tr>
<tr>
<td>Total</td>
<td>n-1</td>
<td>SST</td>
<td></td>
</tr>
</tbody>
</table>
<ol>
<li>Goodness of fit
<ul>
<li>ä¸€å…ƒçº¿æ€§å›å½’ä¸­ coefficient of determination  $R^2 = \frac{SSR}{SST} = 1-\frac{SSE}{SST}$</li>
<li>å¤šå…ƒçº¿æ€§å›å½’ä¸­ adjusted $R^2 = 1-[\frac{n-1}{n-k-1}\times (1-R^2)]$
<ul>
<li>adjusted $R^2$ ä¸€å®šå°äº $R^2$ï¼Œç”šè‡³å¯èƒ½å°äº0</li>
<li>$R^2$ çš„å«ä¹‰æ˜¯å› å˜é‡å˜åŒ–è¢«è§£é‡Šçš„æ¯”ç‡ï¼Œä½†æ˜¯ adjusted $R^2$ å¹¶æ— æ­¤å«ä¹‰</li>
<li>$R^2$ å’Œ adjusted $R^2$ éƒ½ä¸èƒ½è¯´æ˜å›å½’ç³»æ•°æ˜¯å¦æœ‰æ˜¾è‘—æ€§ï¼Œä¹Ÿä¸èƒ½è¯´æ˜æ¨¡å‹æ‹Ÿåˆåº¦çš„æ˜¾è‘—æ€§ï¼Œéœ€è¦é€šè¿‡æ–¹å·®åˆ†æå’Œå»ºè®¾æ£€éªŒæ‰èƒ½å¾—å‡ºç»“è®º</li>
</ul>
</li>
<li>AICï¼šç”¨äºæ¯”è¾ƒå› å˜é‡ç›¸åŒçš„å„ä¸ªæ¨¡å‹çš„æ‹Ÿåˆä¼˜åº¦ AIC =  $n \times ln(\frac{SSE}{n})+2(k+1)$</li>
<li>BICï¼šBIC =  $n \times ln(\frac{SSE}{n})+ln(n)\times(k+1)$
<ul>
<li>é€šå¸¸ BIC &gt; AIC</li>
<li>è‹¥æ›´å…³æ³¨æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ï¼Œç”¨ AICï¼›è‹¥å…³æ³¨æ¨¡å‹çš„æ‹Ÿåˆä¼˜åº¦ï¼Œç”¨ BIC</li>
<li>AIC å’Œ BIC è¶Šå°è¶Šå¥½</li>
</ul>
</li>
</ul>
</li>
<li>Testing joint hypotheses for coefficients
<ul>
<li>å•ä¸ªå›å½’ç³»æ•°ï¼št æ£€éªŒ</li>
<li>è”åˆå‡è®¾æ£€éªŒï¼ˆjoin hypothesis testï¼‰ï¼šF æ£€éªŒ
<ul>
<li>
<p>unrestricted modelï¼š$Y_i = b_0 + b_1 X_{1i} + b_2 X_{2i} + b_3 X_{3i} + b_4 X_{4i} + \epsilon_i$</p>
</li>
<li>
<p>restricted modelï¼šä¾‹å¦‚ $Y_i = b_0 + b_1 X_{1i} + b_2 X_{2i} + \epsilon_i$</p>
</li>
<li>
<p>è”åˆæ£€éªŒï¼š$H_0ï¼šb_3 = b_4 = 0$, $H_a$ï¼š$b_3$ å’Œ $b_4$ ä¸­è‡³å°‘æœ‰ä¸€ä¸ªä¸ç­‰äº0ï¼ˆé™åˆ¶æ¡ä»¶q=2ï¼‰</p>
<p>$$
F = \frac{(SSE_{restricted} - SSE_{unrestricted})/q}{SSE_{unrestricted}/(n-k-1)}
$$</p>
</li>
<li>
<p>å‡è®¾æ£€éªŒæ­¥éª¤</p>
<ol>
<li>State the hypothesis</li>
<li>Identify the appropriate test statistic</li>
<li>Specify the level of significance</li>
<li>State the decision rule</li>
<li>Calculate the test statistic</li>
<li>Make a decision</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="learning-module-3-model-misspecification">Learning Module 3: Model misspecification</h3>
<ol>
<li>Model specification errors æ¨¡å‹è®¾å®šé”™è¯¯
<ul>
<li>æ¨¡å‹è®¾å®š model specification æ˜¯æŒ‡ç¡®å®šå›å½’æ–¹ç¨‹ä¸­é€‰å–çš„å˜é‡ä»¥åŠå˜é‡çš„å‡½æ•°å½¢å¼</li>
<li>æ¨¡å‹è®¾å®šä¸€èˆ¬éµå¾ªä»¥ä¸‹5æ¡åŸåˆ™
<ol>
<li>æ¨¡å‹åº”åŸºäºåŸºæœ¬çš„ç»æµç†è®º</li>
<li>æ¨¡å‹åº”ç²¾ç®€ï¼Œè¦é€‰å–å…³é”®å˜é‡</li>
<li>æ¨¡å‹åº”é€šè¿‡æ ·æœ¬å¤– (out of sample) æ•°æ®æ£€æµ‹ï¼Œä»¥åˆ¤æ–­æ¨¡å‹æ˜¯å¦å¯ä»¥è¢«æ¨å¹¿ä½¿ç”¨</li>
<li>æ¨¡å‹ä¸­å˜é‡çš„å‡½æ•°å½¢å¼åº”ç¬¦åˆå˜é‡æ•°æ®çš„å®é™…ç‰¹å¾</li>
<li>æ¨¡å‹åº”ç¬¦åˆå›å½’å‡è®¾</li>
</ol>
</li>
</ul>
</li>
<li>misspecified functional form
<table>
<thead>
<tr>
<th>Failures in regression functional form</th>
<th>Explanation</th>
<th>Consequence</th>
</tr>
</thead>
<tbody>
<tr>
<td>Omitted variables é—æ¼å˜é‡</td>
<td>é—æ¼äº†ä¸€ä¸ªæˆ–å¤šä¸ªé‡è¦å˜é‡</td>
<td>å¯èƒ½é€ æˆå¼‚æ–¹å·®å’Œåºåˆ—ç›¸å…³</td>
</tr>
<tr>
<td>Inappropriate form of variables</td>
<td></td>
<td></td>
</tr>
<tr>
<td>é”™è¯¯çš„å˜é‡å½¢å¼</td>
<td>å¿½ç•¥äº†éçº¿æ€§å…³ç³»</td>
<td>å¯èƒ½é€ æˆå¼‚æ–¹å·®</td>
</tr>
<tr>
<td>Inappropriate scaling of variables</td>
<td></td>
<td></td>
</tr>
<tr>
<td>æœªä½¿ç”¨ç¼©æ”¾çš„æ•°æ®</td>
<td>å˜é‡å¯èƒ½éœ€è¦é€šè¿‡transformå†æ”¾è¿›æ¨¡å‹ä¸­</td>
<td>å¯èƒ½é€ æˆå¼‚æ–¹å·®å’Œå¤šé‡å…±çº¿æ€§</td>
</tr>
<tr>
<td>Inappropriate pooling of data</td>
<td></td>
<td></td>
</tr>
<tr>
<td>é”™è¯¯èåˆæ¥è‡ªä¸åŒæ ·æœ¬çš„æ•°æ®</td>
<td>æŠŠä¸åŒæ ·æœ¬é›†æ”¾åˆ°ä¸€èµ·å›å½’</td>
<td>å¯èƒ½é€ æˆå¼‚æ–¹å·®å’Œåºåˆ—ç›¸å…³</td>
</tr>
</tbody>
</table>
</li>
<li>Violations of regression assumptions
<table>
<thead>
<tr>
<th></th>
<th>Heteroskedasticity</th>
<th>Serial correlation</th>
<th>Multicolinearity</th>
</tr>
</thead>
<tbody>
<tr>
<td>Description</td>
<td>unconditional å¼‚æ–¹å·®ï¼š æ®‹å·®çš„æ–¹å·®ä¸æ’å®šï¼Œä½†ä¸è‡ªå˜é‡ä¸ç›¸å…³</td>
<td></td>
<td></td>
</tr>
<tr>
<td>conditional å¼‚æ–¹å·®ï¼šæ®‹å·®çš„æ–¹å·®ä¸æ’å®šï¼Œä¸”æ®‹å·®çš„æ–¹å·®ä¸è‡ªå˜é‡ç›¸å…³</td>
<td>æ­£åºåˆ—ç›¸å…³ï¼šå‰ä¸€ä¸ªæ®‹å·®å¤§äº 0ï¼Œåä¸€ä¸ªæ®‹å·®å¤§äº 0 çš„æ¦‚ç‡è¾ƒå¤§ã€‚</td>
<td>ä¸¤ä¸ªæˆ–æ›´å¤šè‡ªå˜é‡ä¹‹é—´é«˜åº¦çº¿æ€§ç›¸å…³</td>
<td></td>
</tr>
<tr>
<td>Consequences</td>
<td>å¯èƒ½ä¼šé€ æˆæ ‡å‡†è¯¯åå°ï¼Œå®¹æ˜“çŠ¯ä¸€ç±»é”™è¯¯</td>
<td>- æ­£åºåˆ—ç›¸å…³ - ä¸€ç±»é”™è¯¯</td>
<td></td>
</tr>
</tbody>
</table>
<ul>
<li>è´Ÿåºåˆ—ç›¸å…³ - äºŒç±»é”™è¯¯</li>
<li>å¦‚æœæ¨¡å‹è‡ªå˜é‡ä¸­ä¸å­˜åœ¨å› å˜é‡çš„æ»åæ€§ï¼Œå•§åºåˆ—ç›¸å…³ä¸å½±å“ç³»æ•°ä¼°è®¡çš„ä¸€è‡´æ€§ï¼›å¦åˆ™ä¼šå¯¼è‡´ç³»æ•°ä¼°è®¡æ— æ•ˆã€‚ | è®¡ç®—çš„æ ‡å‡†è¯¯åå¤§ï¼Œå®¹æ˜“çŠ¯äºŒç±»é”™è¯¯ |
| Testing | - æ•£ç‚¹å›¾</li>
<li>Breusch-Pagan(BP)æ£€éªŒï¼šBP =$ n \times R^2_{res}$, å°†æ®‹å·®çš„å¹³æ–¹ä¸è‡ªå˜é‡åšå›å½’ï¼Œå•å°¾æ£€éªŒï¼Œæ‹’ç»åŸŸåœ¨å³å°¾ | - DW æ£€éªŒï¼ˆä¸€é˜¶åºåˆ—ç›¸å…³ï¼‰</li>
<li>BG æ£€éªŒï¼ˆpé˜¶åºåˆ—ç›¸å…³ï¼‰~$ F_{n-p-k-1,p}$ | $VIF_j$ =$ \frac{1}{1-R^2_j}$
å…¶ä¸­ $ R^2_j$ æ˜¯å°†ç¬¬ j ä¸ªè‡ªå˜é‡ä½œä¸ºå› å˜é‡ï¼Œä¸å…¶ä»–k-1 ä¸ªè‡ªå˜é‡åšçº¿æ€§å›å½’ã€‚VIF &gt; 5 å¯èƒ½å­˜åœ¨å¤šé‡å…±çº¿ï¼Œ&gt;10 ä¸¥é‡å¤šé‡å…±çº¿  |
| Correcting | - robust standard errors</li>
<li>heteroskedasticity-consistent standard errors</li>
<li>White-corrected standard errors | - serial-correlation consistent standard errors</li>
<li>serial correlation and heteroskedasticity adjusted standard errors</li>
<li>Newey-West standard errors</li>
<li>Robust standard errors | - å»æ‰ä¸€ä¸ªæˆ–å¤šä¸ªå…±çº¿æ€§çš„è‡ªå˜é‡</li>
<li>ä»¥æ›¿ä»£å˜é‡æ¥ä»£æ›¿ä¸€ä¸ªå…±çº¿æ€§çš„è‡ªå˜é‡</li>
<li>å¢åŠ æ ·æœ¬å®¹é‡ n |</li>
</ul>
</li>
</ol>
<h3 id="learning-module-4-extensions-of-multiple-regression">Learning Module 4: Extensions of multiple regression</h3>
<ol>
<li>Influence analysis å½±å“åŠ›åˆ†æ
<ul>
<li>å¼ºå½±å“ç‚¹ï¼ˆInfluential observationï¼‰
<ul>
<li>é«˜æ æ†ç‚¹ï¼ˆhigh-leverage pointï¼‰ï¼šæŒ‡è‡ªå˜é‡ä¸ºæå€¼</li>
<li>å¼‚å¸¸å€¼ï¼ˆoutlierï¼‰ï¼šå› å˜é‡ä¸ºæå€¼</li>
</ul>
</li>
<li>æ£€æµ‹æ–¹æ³•æ€»ç»“
<table>
<thead>
<tr>
<th>åç§°</th>
<th>å½±å“æ¥æº</th>
<th>æ£€æµ‹æŒ‡æ ‡</th>
<th>è®¡ç®—æ–¹æ³•</th>
<th>æ£€æµ‹æ–¹æ³•</th>
</tr>
</thead>
<tbody>
<tr>
<td>é«˜æ æ†ç‚¹</td>
<td>è‡ªå˜é‡</td>
<td>æ æ†ç‡ $ h_{ii}$</td>
<td>åº¦é‡æŸä¸ªè‡ªå˜é‡çš„ç¬¬ i ä¸ªè§‚æµ‹å€¼ä¸å…¶ n ä¸ªè§‚æµ‹å€¼å‡å€¼çš„è·ç¦»</td>
<td>$h_{ii} &gt; 3(\frac{k+1}{n})$ï¼Œæ½œåœ¨çš„é«˜æ æ†ç‚¹</td>
</tr>
<tr>
<td>å¼‚å¸¸å€¼</td>
<td>å› å˜é‡</td>
<td>å­¦ç”ŸåŒ–æ®‹å·® $ t_i^*$</td>
<td>1. ç”¨å…¨éƒ¨æ ·æœ¬å»ºæ¨¡ï¼Œå¾—åˆ°æ®‹å·®æ ‡å‡†å·®$s_{e^*}$ï¼Œç„¶åä¾æ¬¡å‰”é™¤ç¬¬iä¸ªæ ·æœ¬é‡æ–°å»ºæ¨¡</td>
<td></td>
</tr>
<tr>
<td>2. $ \epsilon_i^* = Y_i - \hat{Y}_{i^*}$</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>3. $t_{i^<em>} = \epsilon_i^</em>/s_{e^*}=\frac{e_i}{\sqrt{MSE_{(i)}(1-h_{ii})}}\sim t_{n-k-2}$</td>
<td>$</td>
<td>t_i^*</td>
<td>&gt;t $å…³é”®å€¼ï¼Œæ½œåœ¨çš„å¼‚å¸¸å€¼ï¼Œ&gt; 3 åˆ™è®¤å®šä¸ºå¼‚å¸¸å€¼</td>
<td></td>
</tr>
<tr>
<td>å¼ºå½±å“ç‚¹</td>
<td>è‡ªå˜é‡å’Œå› å˜é‡</td>
<td>Cookâ€™s distance $ D_i$</td>
<td>$D_i = \frac{\epsilon_i^2}{k\times MSE}\times \frac{h_{ii}}{(1-h_{ii})^2}$</td>
<td>$D_i &gt; \sqrt{k/n}$ï¼Œå¾ˆå¯èƒ½ä¸ºå¼ºå½±å“ç‚¹</td>
</tr>
</tbody>
</table>
<blockquote>
<p>1ï¼Œ å¾ˆå¯èƒ½
0.5ï¼Œå¯èƒ½ |</p>
</blockquote>
</li>
</ul>
</li>
<li>è™šæ‹Ÿå˜é‡ï¼ˆDummy variablesï¼‰
<ul>
<li>intercept dummyï¼š$Y = b_0 + d_0D+b_1X+\epsilon$</li>
<li>slope dummyï¼š$Y = b_0 + b_1X + d_1DX + \epsilon$</li>
</ul>
</li>
<li>å®šæ€§å› å˜é‡çš„å¤šå…ƒçº¿æ€§å›å½’ - logistic regression
<ul>
<li>odds = P/(1-P)</li>
<li>log odds (or logit) = ln(P/(1-P))</li>
<li>$ln(\frac{P}{1-P}) = b_0 + b_1X_1 + â€¦ + \epsilon$</li>
<li>$P = \frac{1}{1+exp[ -(b_0 + b_1X_1 + â€¦ + \epsilon)]}$</li>
<li>æœ€å¤§ä¼¼ç„¶ä¼°è®¡ MLE è¿›è¡Œå›å½’ç³»æ•°ä¼°è®¡</li>
<li>ä¼¼ç„¶æ¯”æ£€éªŒ LR test æ£€éªŒæ‹Ÿåˆä¼˜åº¦</li>
</ul>
</li>
</ol>
<h3 id="learning-module-5-time-series-analysis">Learning Module 5: Time-series analysis</h3>
<ol>
<li>
<p>Trend models è¶‹åŠ¿æ¨¡å‹</p>
<ul>
<li>Linear trend model: $y_t = b_0 + b_1t + \epsilon_t$</li>
<li>Log-linear trend model (exponential trend): $y_t = e^{b_0 + b_1t}$
<ul>
<li>å¢é•¿ç‡ä¸ºå¸¸æ•°ï¼Œ$y_{t+1}/y_t - 1 = e^{b_1}-1$</li>
</ul>
</li>
<li>å¦‚æœé€‰å–çš„è¶‹åŠ¿æ¨¡å‹èƒ½å¾ˆå¥½åœ°æ¨¡æ‹Ÿæ—¶é—´åºåˆ—ï¼Œé‚£ä¹ˆåº”å½“ç”±æ®‹å·®åºåˆ—ä¸ç›¸å…³ã€‚å¯ç”¨DWæ£€éªŒã€‚</li>
</ul>
</li>
<li>
<p>Autoregressive model è‡ªå›å½’æ¨¡å‹</p>
<ul>
<li>å®šä¹‰
<ul>
<li>AR(1)ï¼š$y_t = b_0 + b_1y_{t-1} + \epsilon_t$</li>
<li>AR(p)ï¼š$y_t = b_0 + b_1y_{t-1} +\dots +b_py_{t-p}+ \epsilon_t$</li>
</ul>
</li>
<li>åæ–¹å·®å¹³ç¨³ covariance stationary
<ul>
<li>å¦‚æœæ—¶é—´åºåˆ—ä¸å¹³ç¨³ï¼Œé‚£ä¹ˆæœ‰å…³å›å½’æ–¹ç¨‹çš„ç³»æ•°ä¼°è®¡æ˜¯æœ‰åçš„ï¼Œç»Ÿè®¡æ¨æ–­éæœ‰æ•ˆ</li>
<li>å¯¹ä¸€ç»„æ—¶é—´åºåˆ—æ•°æ®ï¼Œç¬¬ä¸€æ­¥å°±æ˜¯åˆ¤æ–­æ˜¯å¦å¹³ç¨³</li>
<li>åæ–¹å·®å¹³ç¨³å®šä¹‰ï¼š
<ul>
<li>å‡å€¼å¹³ç¨³ $E(y_t) = \mu$</li>
<li>æ–¹å·®å¹³ç¨³ $Var(y_t) = \sigma^2 &lt; \infty$</li>
<li>ç»“æ„å¹³ç¨³ï¼ˆå‘¨æœŸæ€§ï¼‰$Cov(y_t,y_{t-\tau}) = \gamma (\tau)$</li>
</ul>
</li>
</ul>
</li>
<li>AR æ¨¡å‹åºåˆ—ç›¸å…³æ€§æ£€éªŒ
<ul>
<li>kth-order autocorrelation $(\rho_k) = \frac{cov(x_t,x_{t-k})}{\sigma_x^2}$</li>
<li>æ­¥éª¤
<ul>
<li>æ„å»ºå¹¶ä¼°è®¡ARï¼ˆ1ï¼‰æ¨¡å‹</li>
<li>è®¡ç®—æ¨¡å‹æ®‹å·®ä¹‹é—´çš„è‡ªç›¸å…³ç³»æ•°</li>
<li>æ£€éªŒæ®‹å·®çš„å„é˜¶è‡ªç›¸å…³ç³»æ•°æ˜¯å¦æ˜¾è‘—ä¸ä¸º0</li>
</ul>
</li>
</ul>
</li>
<li>å‡å€¼å¤å½’ Mean reversion
<ul>
<li>å…·æœ‰å‡å€¼å¤å½’ç‰¹æ€§çš„æ—¶é—´åºåˆ—å¤„äºå‡å€¼æ°´å¹³æ—¶ï¼Œå¯¹ä¸‹ä¸€æœŸçš„é¢„æµ‹ä»ç„¶åº”å½“æ˜¯å‡å€¼</li>
<li>å‡å€¼å¤å½’æ°´å¹³ $y_t = \frac{b_0}{1-b_1}$</li>
</ul>
</li>
<li>æ¨¡å‹é¢„æµ‹
<ul>
<li>å¦‚ä½•é€‰æ‹©é¢„æµ‹æ¨¡å‹ï¼šæ ¹æ®é¢„æµ‹è¯¯å·®è¿›è¡Œåˆ¤æ–­</li>
<li>in-sample forecast errorsï¼šå›å½’æ ‡å‡†å·®SEE</li>
<li>out-of-sample forecast errorsï¼šå‡æ–¹è¯¯RMSEï¼Œè¶Šå°è¶Šå¥½</li>
<li>å›å½’ç³»æ•°ä¸ç¨³å®šæ€§ï¼šé€‰å–ä¸åŒæ—¶é—´æ®µçš„å†å²æ•°æ®å¯èƒ½ä¼šå¾—åˆ°ä¸åŒçš„æ¨¡å‹æˆ–å›å½’ç³»æ•°</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Random walk éšæœºæ¸¸èµ°  â€” $b_1 = 1$ çš„AR (1)</p>
<ul>
<li>å®šä¹‰ï¼š$y_t = y_{t-1} + \epsilon_t$ï¼Œ$E(\epsilon_t)=0$, $E(\epsilon_t^2)=\sigma^2$, $E(\epsilon_t \epsilon_s)=0(t \neq s)$</li>
<li>å‡å€¼ä¸å¤å½’ï¼Œæ–¹å·®ä¸æœ‰é™ï¼Œä¸æ»¡è¶³åæ–¹å·®å¹³ç¨³çš„æ¡ä»¶
<ul>
<li>å¤„ç†æ–¹æ³•ï¼šä¸€é˜¶å·®åˆ† first-differencing    $y_t^\prime = \Delta y_t = y_t - y_{t-1}$</li>
<li>å·®åˆ†ååºåˆ—å¹³ç¨³</li>
</ul>
</li>
<li>å«æ¼‚ç§»é¡¹çš„éšæœºæ¸¸èµ°ï¼š$y_t = b_0 + y_{t-1} + \epsilon$,  $b_0 \neq 0$,â€¦.</li>
</ul>
</li>
<li>
<p>Unit Root Test éå¹³ç¨³çš„å•ä½æ ¹æ£€éªŒ</p>
<ul>
<li>å¦‚æœä¸€ä¸ªæ—¶é—´åºåˆ—æœ‰å•ä½æ ¹ï¼Œåˆ™åºåˆ—éå¹³ç¨³ã€‚</li>
<li>å•ä½æ ¹æ£€éªŒåŸºæœ¬æ€æƒ³ï¼šå¦‚æœæœ‰  $|b_1| \geq 1$ï¼Œåˆ™æ—¶é—´åºåˆ—ä¸å¹³ç¨³</li>
<li>Dickey-Fuller æ£€éªŒ
<ul>
<li>$\Delta y_t = b_0 + b_1^\prime y_{t-1} + \epsilon_t$</li>
<li>$H_0: b_1^\prime = 0$ï¼ˆéå¹³ç¨³ï¼Œå…·æœ‰å•ä½æ ¹ï¼‰$H_a: b_1^\prime &lt; 0$</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Moving-average ç§»åŠ¨å¹³å‡æ—¶åºæ¨¡å‹</p>
<ul>
<li>n-period moving average = $(y_t + y_{t-1}+\dots + y_{t-(n-1)})/n$</li>
<li>MA (q) å®šä¹‰ï¼š$y_t = \epsilon_t + \theta_1 \epsilon_{t-1}+\dots+\theta_q\epsilon_{t-q}$, $E(\epsilon_t)=0$, $E(\epsilon_t^2)=\sigma^2$, $cov(\epsilon_t, \epsilon_s) = 0$ for  $t \neq s$</li>
<li>S&amp;P BSE 100 æŒ‡æ•°æ›´é€‚åˆç”¨ MA æ¨¡å‹ï¼ˆç›¸æ¯”äº AR æ¨¡å‹ï¼‰</li>
<li>ç‰¹å¾ï¼šMA (q) æ¨¡å‹çš„å‰ q-autocorrelations æ˜¾è‘—ä¸ç­‰äº0ï¼Œè€Œåçªç„¶å˜æˆ0. è€Œ AR æ¨¡å‹çš„è‡ªç›¸å…³ç³»æ•°æ˜¯é€æ¸å‡å°çš„ã€‚</li>
<li>ARMA æ¨¡å‹ ä»¥æ­¤ç±»æ¨</li>
</ul>
</li>
<li>
<p>å­£èŠ‚æ€§å› ç´ </p>
<ul>
<li>å­˜åœ¨å­£èŠ‚æ€§ç‰¹å¾æ—¶ï¼ŒAR(1) æ®‹å·®é¡¹ä¼šåºåˆ—ç›¸å…³ï¼Œéœ€è¦å°†æ»å p é˜¶çš„æ—¶é—´åºåˆ—ä¹ŸåŠ å…¥æ¨¡å‹ï¼ˆå‡è®¾è€ƒå¯Ÿå¯¹è±¡ä¸ºæ—¶é—´é—´éš”ä¸º p çš„æ•°æ®ï¼‰</li>
</ul>
</li>
<li>
<p>ARCH model è‡ªå›å½’æ¡ä»¶å¼‚æ–¹å·®æ¨¡å‹</p>
<ul>
<li>ARCH(p) å®šä¹‰ï¼š$\epsilon_t^2 = a_0 + a_1 \epsilon^2_{t-1} + \dots + a_p \epsilon^2_{t-p} + u_t$</li>
</ul>
</li>
<li>
<p>å¤šä¸ªæ—¶é—´åºåˆ—çš„å›å½’</p>
<ul>
<li>Cointegrated åæ•´
<table>
<thead>
<tr>
<th>å¹³ç¨³æ€§æ£€éªŒ</th>
<th>ç»“è®ºä¸å¤„ç†æ–¹æ³•</th>
</tr>
</thead>
<tbody>
<tr>
<td>ä¸¤ä¸ªæ—¶é—´åºåˆ—å‡å¹³ç¨³</td>
<td>ç›´æ¥å›å½’å³å¯</td>
</tr>
<tr>
<td>ä¸€ä¸ªå¹³ç¨³ï¼Œä¸€ä¸ªéå¹³ç¨³</td>
<td>ä¸èƒ½å›å½’</td>
</tr>
<tr>
<td>ä¸¤ä¸ªæ—¶é—´åºåˆ—å‡éå¹³ç¨³</td>
<td></td>
</tr>
<tr>
<td>å›å½’åæ®‹å·®é¡¹éå¹³ç¨³</td>
<td>ä¸å­˜åœ¨åæ•´</td>
</tr>
<tr>
<td>ä¸¤ä¸ªæ—¶é—´åºåˆ—å‡éå¹³ç¨³</td>
<td></td>
</tr>
<tr>
<td>å›å½’åæ®‹å·®é¡¹å¹³ç¨³</td>
<td>å­˜åœ¨åæ•´</td>
</tr>
</tbody>
</table>
</li>
<li>Cointegrated åæ•´ â€” å¤šä¸ªå‡å­˜åœ¨å•ä½æ ¹çš„æ—¶é—´åºåˆ—ä¹‹é—´æ˜¯å¦å­˜åœ¨åæ•´å…³ç³»çš„åˆ¤æ–­ï¼š
<ol>
<li>å¤šä¸ªæ—¶é—´åºåˆ—è¿›è¡Œå›å½’</li>
<li>ç”¨ Dickey-Fuller æ£€éªŒæ®‹å·®é¡¹æ˜¯å¦ä¸ºå¹³ç¨³åºåˆ—
<ol>
<li>å¦‚æœæ— æ³•æ‹’ç»åŸå‡è®¾ï¼ˆå³æ®‹å·®é¡¹å­˜åœ¨å•ä½æ ¹ï¼‰ï¼Œé‚£ä¹ˆä¸å­˜åœ¨åæ•´</li>
<li>å¦‚æœæ‹’ç»åŸå‡è®¾ï¼ˆå³æ®‹å·®é¡¹ä¸å­˜åœ¨å•ä½æ ¹ï¼‰ï¼Œé‚£ä¹ˆå­˜åœ¨åæ•´</li>
</ol>
</li>
</ol>
</li>
</ul>
</li>
<li>
<p>æ—¶åºé¢„æµ‹åˆ†ææ­¥éª¤</p>
<p>The following is a step-by-step guide to building a model to predict a time series.</p>
<ol>
<li>Understand the investment problem you have, and make an initial choice of model. One alternative is a regression model that predicts the future behavior of a variable based on hypothesized causal relationships with other variables. Another is a time-series model that attempts to predict the future behavior of a variable based on the past behavior of the same variable.</li>
<li>If you have decided to use a time-series model, compile the time series and plot it to see whether it looks covariance stationary. The plot might show important deviations from covariance stationarity, including the following:
<ol>
<li>a linear trend,</li>
<li>an exponential trend,</li>
<li>seasonality, or</li>
<li>a significant shift in the time series during the sample period (for example, a change in mean or variance).</li>
</ol>
</li>
<li>If you find no significant seasonality or shift in the time series, then perhaps either a linear trend or an exponential trend will be sufficient to model the time series. In that case, take the following steps:
<ol>
<li>Determine whether a linear or exponential trend seems most reasonable (usually by plotting the series).</li>
<li>Estimate the trend.</li>
<li>Compute the residuals.</li>
<li>Use the Durbinâ€“Watson statistic to determine whether the residuals have significant serial correlation. If you find no significant serial correlation in the residuals, then the trend model is sufficient to capture the dynamics of the time series and you can use that model for forecasting.</li>
</ol>
</li>
<li>If you find significant serial correlation in the residuals from the trend model, use a more complex model, such as an autoregressive model. First, however, reexamine whether the time series is covariance stationary. The following is a list of violations of stationarity, along with potential methods to adjust the time series to make it covariance stationary:
<ol>
<li>If the time series has a linear trend, first-difference the time series.</li>
<li>If the time series has an exponential trend, take the natural log of the time series and then first-difference it.</li>
<li>If the time series shifts significantly during the sample period, estimate different time-series models before and after the shift.</li>
<li>If the time series has significant seasonality, include seasonal lags (discussed in Step 7)</li>
</ol>
</li>
<li>After you have successfully transformed a raw time series into a covariance-stationary time series, you can usually model the transformed series with a short autoregression. To decide which autoregressive model to use, take the following steps:
<ol>
<li>Estimate an AR(1) model.</li>
<li>Test to see whether the residuals from this model have significant serial correlation.</li>
<li>If you find no significant serial correlation in the residuals, you can use the AR(1) model to forecast.</li>
</ol>
</li>
<li>If you find significant serial correlation in the residuals, use an AR(2) model and test for significant serial correlation of the residuals of the AR(2) model.
<ol>
<li>If you find no significant serial correlation, use the AR(2) model.</li>
<li>If you find significant serial correlation of the residuals, keep increasing the order of the AR model until the residual serial correlation is no longer significant.</li>
</ol>
</li>
<li>Your next move is to check for seasonality. You can use one of two approaches:
<ol>
<li>Graph the data and check for regular seasonal patterns.</li>
<li>Examine the data to see whether the seasonal autocorrelations of the residuals from an AR model are significant (for example, the fourth auto correlation for quarterly data) and whether the autocorrelations before and after the seasonal autocorrelations are significant. To correct for seasonality, add seasonal lags to your AR model. For example, if you are using quarterly data, you might add the fourth lag of a time series as an additional variable in an AR(1) or an AR(2) model.</li>
</ol>
</li>
<li>Next, test whether the residuals have autoregressive conditional heteroskedasticity. To test for ARCH(1), for example, do the following:
<ol>
<li>Regress the squared residual from your time-series model on a lagged value of the squared residual.</li>
<li>Test whether the coefficient on the squared lagged residual differs significantly from 0.</li>
<li>If the coefficient on the squared lagged residual does not differ significantly from 0, the residuals do not display ARCH and you can rely on the standard errors from your time-series estimates.</li>
<li>If the coefficient on the squared lagged residual does differ significantly from 0, use generalized least squares or other methods to correct for ARCH.</li>
</ol>
</li>
<li>Finally, you may also want to perform tests of the modelâ€™s out-of-sample forecasting performance to see how the modelâ€™s out-of-sample performance compares to its in-sample performance</li>
</ol>
</li>
</ol>
<h3 id="learning-module-6-machine-learning">Learning Module 6: Machine learning</h3>
<ol>
<li>
<p>æœºå™¨å­¦ä¹ æ¨¡å‹ Evaluation</p>
<ol>
<li>overfitting &amp; underfitting</li>
<li>in-sample errors &amp; out of sample errors</li>
<li>out of sample errors
<ol>
<li>bias errorï¼šæ¨¡å‹åœ¨è®­ç»ƒæ ·æœ¬ä¸­çš„åå·®ï¼Œæ¨¡å‹å‡è®¾è¿‡å¤šæ—¶å¯èƒ½å¯¼è‡´</li>
<li>variance errorï¼šéªŒè¯æ ·æœ¬ä»¥åŠæµ‹è¯•æ ·æœ¬ä¸­è¡¨ç°ï¼Œè¿‡é«˜åˆ™è¿‡æ‹Ÿåˆ</li>
<li>base errorï¼šæ•°æ®è‡ªèº«éšæœºæ€§</li>
</ol>
</li>
<li>ä¸€èˆ¬æ¨¡å‹è¶Šå¤æ‚ï¼Œbias errorè¶Šä½ï¼Œvariance errorè¶Šé«˜</li>
</ol>
<p><img src="1%20Quantitative%20Methods%20e84dd0d2cb6140a5a200ff6cf9e94216/Untitled.png" alt="Untitled"></p>
<p><img src="1%20Quantitative%20Methods%20e84dd0d2cb6140a5a200ff6cf9e94216/Untitled%201.png" alt="Untitled"></p>
</li>
<li>
<p>Supervised learning model</p>
<ul>
<li>Penalized regression
<ul>
<li>LASSOï¼šç›®æ ‡å‡½æ•° $\sum_{i=1}^n(Y_i - \hat{Y}<em>i)^2 + \lambda \sum</em>{i=1}^n| \hat{b}_i|$</li>
</ul>
</li>
<li>SVM
<ul>
<li>maximum margin</li>
<li>åˆ†ç±» or å›å½’</li>
</ul>
</li>
<li>KNN
<ul>
<li>è€ƒå¯Ÿè·ç¦»æ–°æ ·æœ¬ç‚¹æœ€è¿‘çš„Kä¸ªæ ·æœ¬ç‚¹ï¼Œå¹¶å°†æ–°æ ·æœ¬ç‚¹å½’ç±»ä¸ºKä¸ªæ ·æœ¬ç‚¹ä¸­å‡ºç°æ¬¡æ•°æœ€å¤šçš„ç±»åˆ«</li>
<li>K çš„å–å€¼ä¸æ˜“å¤ªä½ä¹Ÿä¸å®œè¿‡é«˜ã€‚æ›´é€‚åˆè¾ƒå°‘çš„è§£é‡Šå˜é‡ã€‚</li>
<li>åˆ†ç±»</li>
</ul>
</li>
<li>CART åˆ†ç±»å›å½’æ ‘
<ul>
<li>root node â€”&gt; decision node â€”&gt; terminal node</li>
<li>æ„å»ºCARTçš„å…³é”®æ­¥éª¤æ˜¯ bifurcate åˆ†æ”¯ï¼Œå°†ä¸€ä¸ªèŠ‚ç‚¹æ‹†åˆ†ä¸ºä¸¤ä¸ªå­èŠ‚ç‚¹ã€‚å½“å­èŠ‚ç‚¹çš„è¯¯å·®ä¸çˆ¶èŠ‚ç‚¹çš„è¯¯å·®å°äºé¢„å…ˆè®¾å®šçš„é˜ˆå€¼æ—¶ï¼Œä¸å†è¿›è¡Œåˆ†æ”¯ã€‚</li>
<li>åˆ†ç±» or å›å½’</li>
</ul>
</li>
<li>Ensemble learning and Random forest
<ul>
<li>Voting classifiersï¼šç”±ä¸åŒç®—æ³•çš„æ¨¡å‹ç»„æˆï¼Œé€šè¿‡æŠ•ç¥¨æ¥è¿›è¡Œå†³ç­–</li>
<li>Bootstrap aggregatingï¼ŒBaggingï¼šæœ‰ç›¸åŒçš„å­¦ä¹ æ¨¡å‹ç»„æˆï¼Œé€šè¿‡ bootstrap å¾—åˆ°ç»„åˆæ¨¡å‹
<ul>
<li>bootstrap  æœ‰åŠ©äºé˜²æ­¢è¿‡åº¦æ‹Ÿåˆ</li>
</ul>
</li>
<li>Random forestï¼šå¯¹éšæœºæ£®æ—ä¸­çš„åˆ†ç±»æ ‘è¿›è¡Œåˆ†æ”¯æ—¶ï¼Œéå†æ‰€æœ‰å°šæœªè¿›å…¥åˆ†æ”¯çš„è§£é‡Šå˜é‡çš„ä¸€ä¸ªéšæœºå­é›†</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Unsupervised learning model</p>
<ul>
<li>é™ç»´ï¼šä¸å½±å“æ•°æ®è§£é‡Šèƒ½åŠ›çš„æƒ…å†µä¸‹é™ä½æ•°æ®ç»´åº¦
<ul>
<li>PCAï¼šæŠŠæ‰€æœ‰è§£é‡Šå˜é‡ç»¼åˆåœ¨ä¸€èµ·è¿›è¡Œæ­£äº¤åˆ†è§£ï¼ŒæŒ‰ç…§è§£é‡ŠåŠ›åº¦ä»é«˜åˆ°ä½é€ä¸€åˆ†è§£</li>
</ul>
</li>
<li>èšç±»
<ul>
<li>Hierarchical Clustering åˆ†å±‚èšç±»
<ul>
<li>Divisive Clustering åˆ†è£‚èšç±»ï¼šè‡ªä¸Šè€Œä¸‹</li>
<li>Agglomerative Clustering åˆå¹¶èšç±»ï¼šè‡ªä¸‹è€Œä¸Š
<ul>
<li>dendrograms å›¾</li>
</ul>
</li>
</ul>
</li>
<li>K-means ï¼šè‡ªä¸‹è€Œä¸Š
<ol>
<li>éšæœºäº§ç”Ÿ K ä¸ª centroids è´¨å¿ƒ</li>
<li>è®¡ç®—æ¯ä¸ªæ•°æ®ç‚¹åˆ° K ä¸ªè´¨å¿ƒçš„è·ç¦»ï¼Œå°†æ¯ä¸ªæ•°æ®ç‚¹å½’ç±»ä¸ºè·ç¦»æœ€è¿‘çš„è´¨å¿ƒæ‰€å¯¹åº”çš„ç±»</li>
<li>å°†æ–°çš„è´¨å¿ƒå®šä¹‰ä¸ºå½’ç±»åˆ°åŸè´¨å¿ƒçš„æ•°æ®ç‚¹çš„ä¸­ç‚¹</li>
<li>å¦‚æœæ–°è´¨å¿ƒç›¸æ¯”åŸè´¨å¿ƒå˜åŒ–å¾ˆå°ï¼Œåˆ™åœæ­¢ç®—æ³•ï¼›å¦åˆ™å›åˆ°ç¬¬äºŒæ­¥</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Deep learning model</p>
<ul>
<li>Neural networks ç¥ç»ç½‘ç»œ
<ul>
<li>å¦‚æœ hidden layer å¾ˆå¤šï¼ˆå¦‚è¶…è¿‡20å±‚ï¼‰ï¼Œé‚£ä¹ˆä¹Ÿå«åš deep learning netsï¼ŒDLNs</li>
</ul>
</li>
<li>Deep learning and reinforcement learning
<ul>
<li>reinforcement learning æ˜¯æŒ‡äººå·¥æ™ºèƒ½ï¼ˆagentï¼‰æ ¹æ®å‘¨å›´çš„ç¯å¢ƒé‡‡å–è¡ŒåŠ¨ï¼Œæ¨¡å‹æ ¹æ®è¡ŒåŠ¨çš„ç»“æœç»™äºˆäººå·¥æ™ºèƒ½å¥–åŠ±ï¼ˆrewardï¼‰æˆ–æƒ©ç½šï¼Œä»è€Œå­¦ä¹ è¡ŒåŠ¨æ–¹å¼ã€‚e.g. AlphaGo</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><img src="1%20Quantitative%20Methods%20e84dd0d2cb6140a5a200ff6cf9e94216/Untitled%202.png" alt="Untitled"></p>
<table>
<thead>
<tr>
<th>æ¨¡å‹ç±»åˆ«</th>
<th>åˆ†ç±»</th>
<th>è¿ç»­</th>
<th>åˆ†ç±»æˆ–è¿ç»­</th>
</tr>
</thead>
<tbody>
<tr>
<td>Supervised</td>
<td>Logistic</td>
<td></td>
<td></td>
</tr>
<tr>
<td>SVM</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>KNN</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>CART</td>
<td>Linear regression</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Panelized regression</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Logistic regression</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>CART</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Random forest</td>
<td>Neural network</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Deep learning</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Reinforcement learning</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Unsupervised</td>
<td>PCA</td>
<td></td>
<td></td>
</tr>
<tr>
<td>åˆ†å±‚èšç±»</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>K-means</td>
<td>PCA</td>
<td></td>
<td></td>
</tr>
<tr>
<td>åˆ†å±‚èšç±»</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>K-means</td>
<td>Neural network</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Deep learning</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Reinforcement learning</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="learning-module-7-big-data-projects">Learning Module 7: Big data projects</h3>
<ol>
<li>å¤§æ•°æ®ç‰¹å¾ï¼švolume, variety, velocity, veracityï¼ˆå¯é æ€§ï¼‰</li>
<li>ç»“æ„åŒ–æ•°æ®ä¸éç»“æ„åŒ–æ•°æ®</li>
</ol>
<table>
<thead>
<tr>
<th></th>
<th>ç»“æ„åŒ–æ•°æ®</th>
<th>éç»“æ„åŒ–æ•°æ®</th>
</tr>
</thead>
<tbody>
<tr>
<td>æ˜ç¡®å»ºæ¨¡çš„ç›®æ ‡</td>
<td>ç¡®è®¤æ¨¡å‹çš„è¾“å…¥å’Œè¾“å‡º</td>
<td>æ–‡æœ¬åˆ†æ(text problem formulation)ï¼Œç¡®è®¤æ¨¡å‹çš„è¾“å…¥å’Œè¾“å‡º</td>
</tr>
<tr>
<td>æ•°æ®æ”¶é›†</td>
<td></td>
<td>æ•°æ®æŠ¤ç†(data curation)</td>
</tr>
<tr>
<td>æ•°æ®çš„å‡†å¤‡ä¸æ•´ç†</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<ol>
<li>data preparing/cleaning | - incompleteness error æ•°æ®ä¸å®Œæ•´</li>
</ol>
<ul>
<li>invalidity error æ— æ•ˆé”™è¯¯å€¼</li>
<li>inaccuracy error æ•°æ®ä¸å‡†ç¡®</li>
<li>inconsistency error æ•°æ®ä¸ä¸€è‡´</li>
<li>non-uniformity error éæ ‡å‡†é”™è¯¯</li>
<li>duplication error é‡å¤é”™è¯¯ | - åˆ é™¤ html çš„æ ‡è¯†ç¬¦</li>
<li>åˆ é™¤æ–­ç‚¹ç¬¦å· punctuations</li>
<li>åˆ é™¤æ•°å­—</li>
<li>åˆ é™¤ç©ºç™½ |
| 2. data wrangling/data preprocessing | - extractionï¼šä»å·²æœ‰ç‰¹å¾ä¸­æ„é€ æ–°çš„å˜é‡</li>
<li>aggregationï¼šå°†ä¸¤ä¸ªæˆ–æ›´å¤šå˜é‡åŠ æ€»åå¾—åˆ°ç±»ä¼¼çš„å˜é‡</li>
<li>filtrationï¼šå»æ‰ä¸éœ€è¦çš„è¡Œ</li>
<li>selectionï¼šå»æ‰ä¸éœ€è¦çš„åˆ—</li>
<li>conversionï¼šå°†æ•°æ®è½¬æ¢ä¸ºåˆé€‚çš„ç±»å‹ | - å°†æ‰€æœ‰æ–‡æœ¬è½¬åŒ–ä¸ºå°å†™</li>
<li>åˆ é™¤åœæ­¢è¯ stop wordsï¼Œä¾‹å¦‚ the, is, a</li>
<li>è¯å¹²æå– stemming</li>
<li>è¯å½¢è¿˜åŸ lemmatization</li>
</ul>
<p>â€”&gt; bag-of-words |
|  | å¼‚å¸¸å€¼ trimming or wisorization
normalizationï¼š$\frac{X_i - X_{min}}{X_{max}-X_{min}}$
standardizationï¼š$\frac{X_i-\mu}{\sigma}$ |  |
| æ•°æ®æ¢ç´¢</p>
<ol>
<li>æ¢ç´¢æ€§æ•°æ®åˆ†æ Exploratory data analysis, EDA | é€šè¿‡å¯è§†åŒ–å›¾è¡¨å‘ç°æ•°æ®å…³è” | æ–‡æœ¬æ¢ç´¢(text exploration)</li>
</ol>
<ul>
<li>ç»Ÿè®¡å•æ–‡æœ¬è¯é¢‘ |
| 2. ç‰¹å¾é€‰æ‹© Feature selection | - åå¤è¿­ä»£çš„è¿‡ç¨‹</li>
<li>åœ¨æé«˜æ¨¡å‹è§£é‡ŠåŠ›åº¦å’ŒåŠ å¿«ç®—æ³•è¿è¡Œé€Ÿåº¦ä¸Šè¿›è¡ŒæŠ‰æ‹© | ç²¾ç®€æ–‡æœ¬æ ‡è®°ç¬¦ï¼Œå™ªå£°é€šå¸¸æ˜¯å‡ºç°é¢‘ç‡æœ€é«˜æˆ–æœ€ä½çš„è¯</li>
<li>åˆ©ç”¨é¢‘ç‡åˆ é™¤å™ªå£°ç‰¹å¾</li>
<li>å¡æ–¹æ£€éªŒç­›é€‰ç‰¹å¾</li>
<li>åˆ©ç”¨ mutual information ç­›é€‰ç‰¹å¾ |
| 3. ç‰¹å¾å·¥ç¨‹ Feature Engineer | - é€šè¿‡å·²æœ‰ç‰¹å¾æ¥æ„å»ºæ–°çš„ç‰¹å¾ | - æ ‡è®°æ•°å­—</li>
<li>N-gramï¼šè¯ç»„</li>
<li>å‘½åå®ä½“æŠ€æœ¯ name entity recognition, NERï¼šè¯†åˆ«ä¸“æœ‰åè¯</li>
<li>è¯æ€§ parts of speech, POS |
| è®­ç»ƒæ¨¡å‹</li>
</ul>
<ol>
<li>æ–¹æ³•é€‰æ‹© method selection | - ç›‘ç£æ¨¡å‹ä¸éç›‘ç£æ¨¡å‹çš„é€‰æ‹©</li>
</ol>
<ul>
<li>
<p>æ•°æ®çš„ç±»å‹</p>
</li>
<li>
<p>æ•°æ®çš„å¤§å°ï¼šè§‚æµ‹å€¼æ•°æ®é‡è¾ƒå¤§æ—¶ç”¨ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œç‰¹å¾å€¼è¾ƒå¤šæ—¶ç”¨æ”¯æŒå‘é‡æœº |  |
| 2. æ¨¡å‹è¡¨ç°è¯„ä¼° performance evaluation | - é”™è¯¯åˆ†æ Error analysisï¼ˆä¸‹é¢è¡¨æ ¼ï¼‰</p>
</li>
<li>
<p>ROCï¼Œ Receiver operating characteristic</p>
</li>
<li>
<p>RMSE = $\sqrt{\sum_{i=1}^n\frac{Predicted_i - Actual_i}{n}}$ | åŒå·¦ |
| 3. æ¨¡å‹è°ƒè¯• tuning | bias: æ¨¡å‹è¿‡äºç®€å•ï¼Œæ¬ æ‹Ÿåˆ
varianceï¼šæ¨¡å‹è¿‡äºå¤æ‚ï¼Œè¿‡åº¦æ‹Ÿåˆ |  |</p>
</li>
<li>
<p>Error analysis</p>
<ul>
<li>confusion matrix
<table>
<thead>
<tr>
<th>é¢„æµ‹\çœŸå®</th>
<th>1</th>
<th>0</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>TP</td>
<td>FP ( Type I error)</td>
</tr>
<tr>
<td>0</td>
<td>FN (Type II error)</td>
<td>TN</td>
</tr>
</tbody>
</table>
</li>
<li>Precision (P) = TP / ( TP + FP )
<ul>
<li>æ¨¡å‹é¢„æµ‹ä¸º1çš„æ ·æœ¬ä¸­ï¼Œæœ‰å¤šå°‘çœŸçš„æ˜¯1</li>
</ul>
</li>
<li>Recall (R) = TP / ( TP + FN )
<ul>
<li>çœŸçš„æ˜¯1çš„æ ·æœ¬ä¸­ï¼Œè¢«é¢„æµ‹ä¸º1çš„æœ‰å¤šå°‘</li>
</ul>
</li>
<li>Accuracy = ( TP + TN ) / ( TP + FP + TN + FN )</li>
<li>F1 score = 2<em>P</em>R/(P+R)
<ul>
<li>P å’Œ R çš„è°ƒå’Œå¹³å‡å€¼</li>
<li>å½“æ•°æ®åˆ†ç±»åˆ†å¸ƒä¸å‡åŒ€æ—¶ï¼ŒF1æ¯”accuracyæ›´é€‚ç”¨ï¼Œåˆ†æ•°è¶Šé«˜æ¨¡å‹è¡¨ç°è¶Šå¥½</li>
</ul>
</li>
</ul>
</li>
<li>
<p>ROC</p>
<ul>
<li>False positive rate FPR = FP / ( TN + FP )</li>
<li>True positive rate TPR = TP / ( TP + FN ) = Recall</li>
<li>ROC è¶Šå¾€å·¦ä¸Šå‡¸è¶Šå¥½</li>
</ul>
<p><img src="1%20Quantitative%20Methods%20e84dd0d2cb6140a5a200ff6cf9e94216/Untitled%203.png" alt="Untitled"></p>
</li>
</ul>



<nav class="post-nav fullwidth kai">
  <span class="nav-prev">&larr; <a href="/tech/2024-02-26-linux-command-learning/">linux å‘½ä»¤å­¦ä¹ </a></span>
  <span class="nav-next"><a href="/tech/2024-04-15-cfa-level-ii-economics/">CFA Level II - Economics</a> &rarr;</span>
</nav>



<section class="fullwidth comments">
<script data-src="https://giscus.app/client.js"
  data-repo="yihui/yihui.org"
  data-repo-id="MDEwOlJlcG9zaXRvcnk4MDc3NDg0NA=="
  data-category="Comments"
  data-category-id="DIC_kwDOBNCGvM4COkjD"
  data-mapping="pathname"
  data-strict="1"
  data-reactions-enabled="1"
  data-emit-metadata="0"
  data-input-position="bottom"
  data-theme="preferred_color_scheme"
  data-lang="en"
  data-loading="lazy"
  crossorigin="anonymous"
  defer>
</script>
</section>



</div>
  <footer class="small">
  <script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/fix-toc.min.js" defer></script>
<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/center-img.min.js" defer></script>
<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/alt-title.min.js" defer></script>
<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/right-quote.min.js" defer></script>
<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/heading-anchor.min.js" defer></script>
<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/ol-id.min.js" defer></script>
<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/fullwidth.min.js" defer></script>







  
  
  <hr/>
  
  <p class="nav-bottom">
    <span>Â© <a href="https://jennysong924.github.io">J Song</a> 2024</span>
    <span class="menu-bottom">





<a href="/tech/index.xml" type="application/rss+xml" title="RSS feed">Subscribe</a>

<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="Attribution-NonCommercial-ShareAlike 4.0 International">License</a>
<a href="/search/">Search</a>
<a href="#">Back to top</a>

</span>
  </p>
  
  </footer>
  </body>
</html>

