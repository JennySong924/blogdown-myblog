<!doctype html><html lang=zh-CN data-theme=light><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=4321&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: light)"><meta name=generator content="Hugo 0.123.0"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="CFA Level II - Quantitative methods"><meta itemprop=description content><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="//localhost:4321/imgs/IMG_7868.jpg"><meta itemprop=keywords content="CFA"><meta property="og:type" content="article"><meta property="og:title" content="CFA Level II - Quantitative methods"><meta property="og:description" content><meta property="og:image" content="/imgs/IMG_7868.jpg"><meta property="og:image:width" content="312"><meta property="og:image:height" content="312"><meta property="og:image:type" content="image/jpeg/png/svg/jpg"><meta property="og:url" content="//localhost:4321/post/2024-04-15-cfa-level-ii-quantitative-methods/"><meta property="og:site_name" content="J Song's Blog"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="J Song"><meta property="article:published_time" content="2024-04-15 00:00:00 +0000 UTC"><meta property="article:modified_time" content="2024-04-15 00:00:00 +0000 UTC"><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/font-awesome/6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/animate.css/3.1.1/animate.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.css><link rel=stylesheet href=/css/main.css><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":false,"isHome":false,"isPage":true,"math":{"js":{"file":"es5/tex-mml-chtml.js","name":"mathjax","version":"3.2.0"},"render":"mathjax"},"path":"2024-04-15-cfa-level-ii-quantitative-methods","permalink":"//localhost:4321/post/2024-04-15-cfa-level-ii-quantitative-methods/","title":"CFA Level II - Quantitative methods","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>CFA Level II - Quantitative methods - J Song's Blog</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage class=use-motion><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>J Song's Blog</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description>随便写写的个人小站</p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/home/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>主页</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>文章
<span class=badge>1</span></a></li><li class="menu-item menu-item-thoughts"><a href=/thoughts/ class=hvr-icon-pulse rel=section><i class="fa fa-pen hvr-icon"></i>随笔</a></li><li class="menu-item menu-item-book-list"><a href=/book-list class=hvr-icon-pulse rel=section><i class="fa fa-book hvr-icon"></i>书单</a></li><li class="menu-item menu-item-movie-list"><a href=/movie-list class=hvr-icon-pulse rel=section><i class="fa fa-film hvr-icon"></i>片单</a></li><li class="menu-item menu-item-"><a href=/about class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于我</a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><ul><li><a href=#learning-module-1-basics-of-multiple-regression-and-underlying-assumptions>Learning Module 1: Basics of multiple regression and underlying assumptions</a></li><li><a href=#learning-module-2-evaluating-regression-model-fit-and-interpreting-model-results>Learning Module 2: Evaluating regression model fit and interpreting model results</a></li><li><a href=#learning-module-3-model-misspecification>Learning Module 3: Model misspecification</a></li><li><a href=#learning-module-4-extensions-of-multiple-regression>Learning Module 4: Extensions of multiple regression</a></li><li><a href=#learning-module-5-time-series-analysis>Learning Module 5: Time-series analysis</a></li><li><a href=#learning-module-6-machine-learning>Learning Module 6: Machine learning</a></li><li><a href=#learning-module-7-big-data-projects>Learning Module 7: Big data projects</a></li></ul></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt="J Song" src=/imgs/img-lazy-loading.gif data-src=/imgs/IMG_7868.jpg><p class=site-author-name itemprop=name>J Song</p><div class=site-description itemprop=description></div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>1</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>14</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>16</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/JennySong924/ title="Github → https://github.com/JennySong924/" rel=noopener class=hvr-icon-pulse target=_blank><i class="fab fa-github fa-fw hvr-icon"></i>
Github
</a></span><span class=links-of-social-item><a href=mailto:songj0924@gmail.com title="E-Mail → mailto:songj0924@gmail.com" rel=noopener class=hvr-icon-pulse target=_blank><i class="fa fa-envelope fa-fw hvr-icon"></i>
E-Mail
</a></span><span class=links-of-social-item><a href="https://www.douban.com/people/193870143/?_i=44606619RFdp21" title="豆瓣 → https://www.douban.com/people/193870143/?_i=44606619RFdp21" rel=noopener class=hvr-icon-pulse target=_blank><i class="fa fa-book fa-fw hvr-icon"></i>
豆瓣</a></span></div></div></div></div><div id=siteinfo-card-widget class=sidebar-card-widget><div class=item-headline><i class="fas fa-chart-line"></i>
<span>网站资讯</span></div><div class=siteinfo><div class=siteinfo-item><div class=item-name><i class="fa-solid fa-calendar-check"></i>已运行：</div><div class=item-count id=runTimes data-publishdate=2024-03-31T00:00:00+00:00></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-font"></i>总字数：</div><div class=item-count id=wordsCount data-count=165></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-mug-hot"></i>阅读约：</div><div class=item-count id=readTimes data-times=1></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-clock-rotate-left"></i>最后更新于：</div><div class=item-count id=last-push-date data-lastpushdate=2024-03-31T00:00:00+00:00></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=goto-gtranslate class=button title=多语言翻译><i class="fas fa-globe"></i></div><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><a role=button class="book-mark-link book-mark-link-fixed"></a><a href=https://github.com/hugo-next rel="noopener external nofollow noreferrer" target=_blank title="Follow me on GitHub" class="exturl github-corner"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg>
</a><script type=text/javascript src=//sidecar.gitter.im/dist/sidecar.v1.js async></script><script type=text/javascript>((window.gitter={}).chat={}).options={room:"hugo-next/community"}</script><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=//localhost:4321/post/2024-04-15-cfa-level-ii-quantitative-methods/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/IMG_7868.jpg"><meta itemprop=name content="J Song"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="J Song"><meta itemprop=description content></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="CFA Level II - Quantitative methods"><meta itemprop=description content="Learning Module 1: Basics of multiple regression and underlying assumptions Uses of multiple linear regression Multiple linear regression is used to model the linear relationship between one dependent variable and two or more independent variables. In practice, multiple regressions are used to explain relationships between financial variables, to test existing theories, or to make forecasts. The basics of multiple regression The regression process covers several decisions the analyst must make,"></span><header class=post-header><h1 class=post-title itemprop="name headline">CFA Level II - Quantitative methods</h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i>
</span><span class=post-meta-item-text title=发表于>发表于：
</span><time title="创建时间：2024-04-15 00:00:00 +0000 UTC" itemprop="dateCreated datePublished" datetime="2024-04-15 00:00:00 +0000 UTC">2024-04-15
</time></span><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-folder-open"></i>
</span><span class=post-meta-item-text title=分类于>分类于：
</span><span itemprop=about itemscope itemtype=http://schema.org/Thing><a href=/categories/%E7%BB%8F%E6%B5%8E itemprop=url rel=index><span itemprop=name>技术文章/经济</span></a></span></span></div><div class=post-meta-items></div></div></header><div class=post-body itemprop=articleBody><h3 id=learning-module-1-basics-of-multiple-regression-and-underlying-assumptions>Learning Module 1: Basics of multiple regression and underlying assumptions
<a class=header-anchor href=#learning-module-1-basics-of-multiple-regression-and-underlying-assumptions></a></h3><ol><li>Uses of multiple linear regression<ul><li>Multiple linear regression is used to model the linear relationship between one dependent variable and two or more independent variables.</li><li>In practice, multiple regressions are used to explain relationships between financial variables, to test existing theories, or to make forecasts.</li></ul></li><li>The basics of multiple regression<ul><li><p>The regression process covers several decisions the analyst must make, such as identifying the dependent and independent variables, selecting the appropriate regression model, testing if the assumptions behind linear regression are satisfied, examining goodness of fit, and making needed adjustments.</p></li><li><p>The regression process</p><p><img src=/imgs/img-lazy-loading.gif data-src=1%20Quantitative%20Methods%20e84dd0d2cb6140a5a200ff6cf9e94216/quantitative-methods.drawio-3.png alt=quantitative-methods.drawio-3.png></p></li><li><p>A multiple regression model is represented by the following equation:
<code>$Y_i = b_0 + b_1X_{1i} + b_2X_{2i} + \dots + b_kX_{ki} + \epsilon_i, i=1,2,3,...,n$</code>
where <code>$Y$</code> is the dependent variable, <code>$Xs$</code> are the independent variables from $1$ to <code>$k$</code>, and the model is estimated using $n$ observations</p></li><li><p>Coefficient $b_0$ is the model’s “intercept” representing the expected value of <code>$Y$</code> if all independent variables are zero.</p></li><li><p>Parameters <code>$b_1$</code> to <code>$b_k$</code> are the slope coefficients (or partial regression coefficients) for independent variables <code>$X_1$</code> to <code>$X_k$</code>. Slope coefficient <code>$b_j$</code> describes the impact of independent variable <code>$X_j$</code> on <code>$Y$</code>, holding all the other independent variables constant.</p></li></ul></li><li>Assumptions underlying multiple linear regression<table><thead><tr><th>Assumptions</th><th>Description</th><th>Diagnose</th></tr></thead><tbody><tr><td>Linearity</td><td>The relationship between the dependent variable and the independent variables is linear.</td><td>Scatterplots of dependent versus and independent variables (pairs plot) — also identify extreme values and outliers</td></tr><tr><td>Homoskedasticity</td><td>The variance of the regression residuals is the same for all observations.</td><td>Scatter plot of residuals against the dependent variable</td></tr><tr><td>Independence of errors</td><td>The observations are independent of one another. This implies the regression residuals are uncorrelated across observations.</td><td>Scatter plot of residuals against the dependent variable</td></tr><tr><td>Normality</td><td>The regression residuals are normally distributed.</td><td>Normal Q-Q plot</td></tr><tr><td>Independence of independent variables</td><td>1. Independent variables are not random.</td><td></td></tr><tr><td>2. There is no exact linear relation between two or more of the independent variables or combinations of the independent variables.</td><td></td><td></td></tr></tbody></table><ul><li><p>Fat-tailed Q-Q plot</p><p><img src=/imgs/img-lazy-loading.gif data-src=1%20Quantitative%20Methods%20e84dd0d2cb6140a5a200ff6cf9e94216/Screenshot_2024-04-07_at_13.18.05.png alt="Screenshot 2024-04-07 at 13.18.05.png"></p></li></ul></li></ol><h3 id=learning-module-2-evaluating-regression-model-fit-and-interpreting-model-results>Learning Module 2: Evaluating regression model fit and interpreting model results
<a class=header-anchor href=#learning-module-2-evaluating-regression-model-fit-and-interpreting-model-results></a></h3><ol><li>Analysis of variance, ANOVA 方差分析</li></ol><table><thead><tr><th></th><th>degrees of freedom, df</th><th>sum of squares, SS</th><th>mean sum of squares, MS</th></tr></thead><tbody><tr><td>Regression</td><td>k</td><td>SSR</td><td>MSR = SSR/k</td></tr><tr><td>Error</td><td>n-k-1</td><td>SSE</td><td>MSE = SSE/(n-k-1)</td></tr><tr><td>Total</td><td>n-1</td><td>SST</td><td></td></tr></tbody></table><ol><li>Goodness of fit<ul><li>一元线性回归中 coefficient of determination <code>$R^2 = \frac{SSR}{SST} = 1-\frac{SSE}{SST}$</code></li><li>多元线性回归中 adjusted <code>$R^2 = 1-[\frac{n-1}{n-k-1}\times (1-R^2)]$</code><ul><li>adjusted <code>$R^2$</code> 一定小于 <code>$R^2$</code>，甚至可能小于0</li><li><code>$R^2$</code> 的含义是因变量变化被解释的比率，但是 adjusted $R^2$ 并无此含义</li><li><code>$R^2$</code> 和 adjusted <code>$R^2$</code> 都不能说明回归系数是否有显著性，也不能说明模型拟合度的显著性，需要通过方差分析和建设检验才能得出结论</li></ul></li><li>AIC：用于比较因变量相同的各个模型的拟合优度 AIC = <code>$n \times ln(\frac{SSE}{n})+2(k+1)$</code></li><li>BIC：BIC = <code>$n \times ln(\frac{SSE}{n})+ln(n)\times(k+1)$</code><ul><li>通常 BIC > AIC</li><li>若更关注模型的预测能力，用 AIC；若关注模型的拟合优度，用 BIC</li><li>AIC 和 BIC 越小越好</li></ul></li></ul></li><li>Testing joint hypotheses for coefficients<ul><li>单个回归系数：t 检验</li><li>联合假设检验（join hypothesis test）：F 检验<ul><li><p>unrestricted model：<code>$Y_i = b_0 + b_1 X_{1i} + b_2 X_{2i} + b_3 X_{3i} + b_4 X_{4i} + \epsilon_i$</code></p></li><li><p>restricted model：例如 <code>$Y_i = b_0 + b_1 X_{1i} + b_2 X_{2i} + \epsilon_i$</code></p></li><li><p>联合检验：<code>$H_0：b_3 = b_4 = 0$</code>, <code>$H_a$</code>：<code>$b_3$</code> 和 <code>$b_4$</code> 中至少有一个不等于0（限制条件q=2）</p><p><code>$$ F = \frac{(SSE_{restricted} - SSE_{unrestricted})/q}{SSE_{unrestricted}/(n-k-1)} $$</code></p></li><li><p>假设检验步骤</p><ol><li>State the hypothesis</li><li>Identify the appropriate test statistic</li><li>Specify the level of significance</li><li>State the decision rule</li><li>Calculate the test statistic</li><li>Make a decision</li></ol></li></ul></li></ul></li></ol><h3 id=learning-module-3-model-misspecification>Learning Module 3: Model misspecification
<a class=header-anchor href=#learning-module-3-model-misspecification></a></h3><ol><li><p>Model specification errors 模型设定错误</p><ul><li>模型设定 model specification 是指确定回归方程中选取的变量以及变量的函数形式</li><li>模型设定一般遵循以下5条原则<ol><li>模型应基于基本的经济理论</li><li>模型应精简，要选取关键变量</li><li>模型应通过样本外 (out of sample) 数据检测，以判断模型是否可以被推广使用</li><li>模型中变量的函数形式应符合变量数据的实际特征</li><li>模型应符合回归假设</li></ol></li></ul></li><li><p>misspecified functional form</p><table><thead><tr><th>Failures in regression functional form</th><th>Explanation</th><th>Consequence</th></tr></thead><tbody><tr><td>Omitted variables 遗漏变量</td><td>遗漏了一个或多个重要变量</td><td>可能造成异方差和序列相关</td></tr><tr><td>Inappropriate form of variables</td><td></td><td></td></tr><tr><td>错误的变量形式</td><td>忽略了非线性关系</td><td>可能造成异方差</td></tr><tr><td>Inappropriate scaling of variables</td><td></td><td></td></tr><tr><td>未使用缩放的数据</td><td>变量可能需要通过transform再放进模型中</td><td>可能造成异方差和多重共线性</td></tr><tr><td>Inappropriate pooling of data</td><td></td><td></td></tr><tr><td>错误融合来自不同样本的数据</td><td>把不同样本集放到一起回归</td><td>可能造成异方差和序列相关</td></tr></tbody></table></li><li><p>Violations of regression assumptions</p></li></ol><table><thead><tr><th></th><th>Heteroskedasticity</th><th>Serial correlation</th><th>Multicolinearity</th></tr></thead><tbody><tr><td>Description</td><td>unconditional 异方差： 残差的方差不恒定，但与自变量不相关</td><td></td><td></td></tr><tr><td>conditional 异方差：残差的方差不恒定，且残差的方差与自变量相关</td><td>正序列相关：前一个残差大于 0，后一个残差大于 0 的概率较大。</td><td>两个或更多自变量之间高度线性相关</td><td></td></tr><tr><td>Consequences</td><td>可能会造成标准误偏小，容易犯一类错误</td><td>- 正序列相关 - 一类错误<br>- 负序列相关 - 二类错误<br>- 如果模型自变量中不存在因变量的滞后性，啧序列相关不影响系数估计的一致性；否则会导致系数估计无效。</td><td>计算的标准误偏大，容易犯二类错误</td></tr><tr><td>Testing</td><td>- 散点图<br>- Breusch-Pagan(BP)检验：BP =<code>$ n \times R^2_{res}$</code>, 将残差的平方与自变量做回归，单尾检验，拒绝域在右尾</td><td>- DW 检验（一阶序列相关）<br>- BG 检验（p阶序列相关）~<code>$ F_{n-p-k-1,p}$</code></td><td><code>$VIF_j$</code> =<code>$ \frac{1}{1-R^2_j}$</code><br>其中 <code>$ R^2_j$</code> 是将第 j 个自变量作为因变量，与其他k-1 个自变量做线性回归。VIF > 5 可能存在多重共线，>10 严重多重共线</td></tr><tr><td>Correcting</td><td>- robust standard errors<br>- heteroskedasticity-consistent standard errors<br>- White-corrected standard errors</td><td>- serial-correlation consistent standard errors<br>- serial correlation and heteroskedasticity adjusted standard errors<br>- Newey-West standard errors<br>- Robust standard errors</td><td>- 去掉一个或多个共线性的自变量<br>- 以替代变量来代替一个共线性的自变量<br>- 增加样本容量 n</td></tr></tbody></table><h3 id=learning-module-4-extensions-of-multiple-regression>Learning Module 4: Extensions of multiple regression
<a class=header-anchor href=#learning-module-4-extensions-of-multiple-regression></a></h3><ol><li>Influence analysis 影响力分析<ul><li>强影响点（Influential observation）<ul><li>高杠杆点（high-leverage point）：指自变量为极值</li><li>异常值（outlier）：因变量为极值</li></ul></li><li>检测方法总结</li></ul></li></ol><table><thead><tr><th>名称</th><th>影响来源</th><th>检测指标</th><th>计算方法</th><th>检测方法</th></tr></thead><tbody><tr><td>高杠杆点</td><td>自变量</td><td>杠杆率 $ h_{ii}$</td><td>度量某个自变量的第 i 个观测值与其 n 个观测值均值的距离</td><td><code>$h_{ii} > 3(\frac{k+1}{n})$</code>，潜在的高杠杆点</td></tr><tr><td>异常值</td><td>因变量</td><td>学生化残差 <code>$ t_i^*$</code></td><td>1. 用全部样本建模，得到残差标准差<code>$s_{e^*}$</code>，然后依次剔除第i个样本重新建模<br>2. <code>$ \epsilon_i^* = Y_i - \hat{Y}_{i^*}$</code><br>3. <code>$t_{i^*} = \epsilon_i^*/s_{e^*}=\frac{e_i}{\sqrt{MSE_{(i)}(1-h_{ii})}}\sim t_{n-k-2}$</code></td><td><code>$ |t_i^*|>t $</code>关键值，潜在的异常值，> 3 则认定为异常值</td></tr><tr><td>强影响点</td><td>自变量和因变量</td><td>Cook’s distance <code>$ D_i$</code></td><td><code>$D_i = \frac{\epsilon_i^2}{k\times MSE}\times \frac{h_{ii}}{(1-h_{ii})^2}$</code></td><td><code>$D_i > \sqrt{k/n}$</code>，很可能为强影响点<br>> 1， 很可能<br>> 0.5，可能</td></tr></tbody></table><ol start=2><li>虚拟变量（Dummy variables）<ul><li>intercept dummy：<code>$Y = b_0 + d_0D+b_1X+\epsilon$</code></li><li>slope dummy：<code>$Y = b_0 + b_1X + d_1DX + \epsilon$</code></li></ul></li><li>定性因变量的多元线性回归 - logistic regression<ul><li>odds = P/(1-P)</li><li>log odds (or logit) = ln(P/(1-P))</li><li><code>$ln(\frac{P}{1-P}) = b_0 + b_1X_1 + … + \epsilon$</code></li><li><code>$P = \frac{1}{1+exp[ -(b_0 + b_1X_1 + … + \epsilon)]}$</code></li><li>最大似然估计 MLE 进行回归系数估计</li><li>似然比检验 LR test 检验拟合优度</li></ul></li></ol><h3 id=learning-module-5-time-series-analysis>Learning Module 5: Time-series analysis
<a class=header-anchor href=#learning-module-5-time-series-analysis></a></h3><ol><li><p>Trend models 趋势模型</p><ul><li>Linear trend model: <code>$y_t = b_0 + b_1t + \epsilon_t$</code></li><li>Log-linear trend model (exponential trend): <code>$y_t = e^{b_0 + b_1t}$</code><ul><li>增长率为常数，<code>$y_{t+1}/y_t - 1 = e^{b_1}-1$</code></li></ul></li><li>如果选取的趋势模型能很好地模拟时间序列，那么应当由残差序列不相关。可用DW检验。</li></ul></li><li><p>Autoregressive model 自回归模型</p><ul><li>定义<ul><li>AR(1)：<code>$y_t = b_0 + b_1y_{t-1} + \epsilon_t$</code></li><li>AR(p)：<code>$y_t = b_0 + b_1y_{t-1} +\dots +b_py_{t-p}+ \epsilon_t$</code></li></ul></li><li>协方差平稳 covariance stationary<ul><li>如果时间序列不平稳，那么有关回归方程的系数估计是有偏的，统计推断非有效</li><li>对一组时间序列数据，第一步就是判断是否平稳</li><li>协方差平稳定义：<ul><li>均值平稳 <code>$E(y_t) = \mu$</code></li><li>方差平稳 <code>$Var(y_t) = \sigma^2 &lt; \infty$</code></li><li>结构平稳（周期性）<code>$Cov(y_t,y_{t-\tau}) = \gamma (\tau)$</code></li></ul></li></ul></li><li>AR 模型序列相关性检验<ul><li>kth-order autocorrelation <code>$(\rho_k) = \frac{cov(x_t,x_{t-k})}{\sigma_x^2}$</code></li><li>步骤<ul><li>构建并估计AR（1）模型</li><li>计算模型残差之间的自相关系数</li><li>检验残差的各阶自相关系数是否显著不为0</li></ul></li></ul></li><li>均值复归 Mean reversion<ul><li>具有均值复归特性的时间序列处于均值水平时，对下一期的预测仍然应当是均值</li><li>均值复归水平 <code>$y_t = \frac{b_0}{1-b_1}$</code></li></ul></li><li>模型预测<ul><li>如何选择预测模型：根据预测误差进行判断</li><li>in-sample forecast errors：回归标准差SEE</li><li>out-of-sample forecast errors：均方误RMSE，越小越好</li><li>回归系数不稳定性：选取不同时间段的历史数据可能会得到不同的模型或回归系数</li></ul></li></ul></li><li><p>Random walk 随机游走 — <code>$b_1 = 1$</code> 的AR (1)</p><ul><li>定义：<code>$y_t = y_{t-1} + \epsilon_t$</code>，<code>$E(\epsilon_t)=0$</code>, <code>$E(\epsilon_t^2)=\sigma^2$</code>, <code>$E(\epsilon_t \epsilon_s)=0(t \neq s)$</code></li><li>均值不复归，方差不有限，不满足协方差平稳的条件<ul><li>处理方法：一阶差分 first-differencing <code>$y_t^\prime = \Delta y_t = y_t - y_{t-1}$</code></li><li>差分后序列平稳</li></ul></li><li>含漂移项的随机游走：<code>$y_t = b_0 + y_{t-1} + \epsilon$</code>, <code>$b_0 \neq 0$</code>,….</li></ul></li><li><p>Unit Root Test 非平稳的单位根检验</p><ul><li>如果一个时间序列有单位根，则序列非平稳。</li><li>单位根检验基本思想：如果有 <code>$|b_1| \geq 1$</code>，则时间序列不平稳</li><li>Dickey-Fuller 检验<ul><li><code>$\Delta y_t = b_0 + b_1^\prime y_{t-1} + \epsilon_t$</code></li><li><code>$H_0: b_1^\prime = 0$</code>（非平稳，具有单位根）<code>$H_a: b_1^\prime &lt; 0$</code></li></ul></li></ul></li><li><p>Moving-average 移动平均时序模型</p><ul><li>n-period moving average = <code>$(y_t + y_{t-1}+\dots + y_{t-(n-1)})/n$</code></li><li>MA (q) 定义：<code>$y_t = \epsilon_t + \theta_1 \epsilon_{t-1}+\dots+\theta_q\epsilon_{t-q}$</code>, <code>$E(\epsilon_t)=0$</code>, <code>$E(\epsilon_t^2)=\sigma^2$</code>, <code>$cov(\epsilon_t, \epsilon_s) = 0$</code> for <code>$t \neq s$</code></li><li>S&amp;P BSE 100 指数更适合用 MA 模型（相比于 AR 模型）</li><li>特征：MA (q) 模型的前 q-autocorrelations 显著不等于0，而后突然变成0. 而 AR 模型的自相关系数是逐渐减小的。</li><li>ARMA 模型 以此类推</li></ul></li><li><p>季节性因素</p><ul><li>存在季节性特征时，AR(1) 残差项会序列相关，需要将滞后 p 阶的时间序列也加入模型（假设考察对象为时间间隔为 p 的数据）</li></ul></li><li><p>ARCH model 自回归条件异方差模型</p><ul><li>ARCH(p) 定义：<code>$\epsilon_t^2 = a_0 + a_1 \epsilon^2_{t-1} + \dots + a_p \epsilon^2_{t-p} + u_t$</code></li></ul></li><li><p>多个时间序列的回归</p><ul><li><p>Cointegrated 协整</p><table><thead><tr><th>平稳性检验</th><th>结论与处理方法</th></tr></thead><tbody><tr><td>两个时间序列均平稳</td><td>直接回归即可</td></tr><tr><td>一个平稳，一个非平稳</td><td>不能回归</td></tr><tr><td>两个时间序列均非平稳<br>回归后残差项非平稳</td><td>不存在协整</td></tr><tr><td>两个时间序列均非平稳<br>回归后残差项平稳</td><td>存在协整</td></tr></tbody></table></li><li><p>Cointegrated 协整 — 多个均存在单位根的时间序列之间是否存在协整关系的判断：</p><ol><li>多个时间序列进行回归</li><li>用 Dickey-Fuller 检验残差项是否为平稳序列<ol><li>如果无法拒绝原假设（即残差项存在单位根），那么不存在协整</li><li>如果拒绝原假设（即残差项不存在单位根），那么存在协整</li></ol></li></ol></li></ul></li><li><p>时序预测分析步骤</p><p>The following is a step-by-step guide to building a model to predict a time series.</p><ol><li>Understand the investment problem you have, and make an initial choice of model. One alternative is a regression model that predicts the future behavior of a variable based on hypothesized causal relationships with other variables. Another is a time-series model that attempts to predict the future behavior of a variable based on the past behavior of the same variable.</li><li>If you have decided to use a time-series model, compile the time series and plot it to see whether it looks covariance stationary. The plot might show important deviations from covariance stationarity, including the following:<ol><li>a linear trend,</li><li>an exponential trend,</li><li>seasonality, or</li><li>a significant shift in the time series during the sample period (for example, a change in mean or variance).</li></ol></li><li>If you find no significant seasonality or shift in the time series, then perhaps either a linear trend or an exponential trend will be sufficient to model the time series. In that case, take the following steps:<ol><li>Determine whether a linear or exponential trend seems most reasonable (usually by plotting the series).</li><li>Estimate the trend.</li><li>Compute the residuals.</li><li>Use the Durbin–Watson statistic to determine whether the residuals have significant serial correlation. If you find no significant serial correlation in the residuals, then the trend model is sufficient to capture the dynamics of the time series and you can use that model for forecasting.</li></ol></li><li>If you find significant serial correlation in the residuals from the trend model, use a more complex model, such as an autoregressive model. First, however, reexamine whether the time series is covariance stationary. The following is a list of violations of stationarity, along with potential methods to adjust the time series to make it covariance stationary:<ol><li>If the time series has a linear trend, first-difference the time series.</li><li>If the time series has an exponential trend, take the natural log of the time series and then first-difference it.</li><li>If the time series shifts significantly during the sample period, estimate different time-series models before and after the shift.</li><li>If the time series has significant seasonality, include seasonal lags (discussed in Step 7)</li></ol></li><li>After you have successfully transformed a raw time series into a covariance-stationary time series, you can usually model the transformed series with a short autoregression. To decide which autoregressive model to use, take the following steps:<ol><li>Estimate an AR(1) model.</li><li>Test to see whether the residuals from this model have significant serial correlation.</li><li>If you find no significant serial correlation in the residuals, you can use the AR(1) model to forecast.</li></ol></li><li>If you find significant serial correlation in the residuals, use an AR(2) model and test for significant serial correlation of the residuals of the AR(2) model.<ol><li>If you find no significant serial correlation, use the AR(2) model.</li><li>If you find significant serial correlation of the residuals, keep increasing the order of the AR model until the residual serial correlation is no longer significant.</li></ol></li><li>Your next move is to check for seasonality. You can use one of two approaches:<ol><li>Graph the data and check for regular seasonal patterns.</li><li>Examine the data to see whether the seasonal autocorrelations of the residuals from an AR model are significant (for example, the fourth auto correlation for quarterly data) and whether the autocorrelations before and after the seasonal autocorrelations are significant. To correct for seasonality, add seasonal lags to your AR model. For example, if you are using quarterly data, you might add the fourth lag of a time series as an additional variable in an AR(1) or an AR(2) model.</li></ol></li><li>Next, test whether the residuals have autoregressive conditional heteroskedasticity. To test for ARCH(1), for example, do the following:<ol><li>Regress the squared residual from your time-series model on a lagged value of the squared residual.</li><li>Test whether the coefficient on the squared lagged residual differs significantly from 0.</li><li>If the coefficient on the squared lagged residual does not differ significantly from 0, the residuals do not display ARCH and you can rely on the standard errors from your time-series estimates.</li><li>If the coefficient on the squared lagged residual does differ significantly from 0, use generalized least squares or other methods to correct for ARCH.</li></ol></li><li>Finally, you may also want to perform tests of the model’s out-of-sample forecasting performance to see how the model’s out-of-sample performance compares to its in-sample performance</li></ol></li></ol><h3 id=learning-module-6-machine-learning>Learning Module 6: Machine learning
<a class=header-anchor href=#learning-module-6-machine-learning></a></h3><ol><li><p>机器学习模型 Evaluation</p><ol><li>overfitting & underfitting</li><li>in-sample errors & out of sample errors</li><li>out of sample errors<ol><li>bias error：模型在训练样本中的偏差，模型假设过多时可能导致</li><li>variance error：验证样本以及测试样本中表现，过高则过拟合</li><li>base error：数据自身随机性</li></ol></li><li>一般模型越复杂，bias error越低，variance error越高</li></ol><p><img src=/imgs/img-lazy-loading.gif data-src=1%20Quantitative%20Methods%20e84dd0d2cb6140a5a200ff6cf9e94216/Untitled.png alt=Untitled></p><p><img src=/imgs/img-lazy-loading.gif data-src=1%20Quantitative%20Methods%20e84dd0d2cb6140a5a200ff6cf9e94216/Untitled%201.png alt=Untitled></p></li><li><p>Supervised learning model</p><ul><li>Penalized regression<ul><li>LASSO：目标函数 <code>$\sum_{i=1}^n(Y_i - \hat{Y}_i)^2 + \lambda \sum_{i=1}^n| \hat{b}_i|$</code></li></ul></li><li>SVM<ul><li>maximum margin</li><li>分类 or 回归</li></ul></li><li>KNN<ul><li>考察距离新样本点最近的K个样本点，并将新样本点归类为K个样本点中出现次数最多的类别</li><li>K 的取值不易太低也不宜过高。更适合较少的解释变量。</li><li>分类</li></ul></li><li>CART 分类回归树<ul><li>root node —> decision node —> terminal node</li><li>构建CART的关键步骤是 bifurcate 分支，将一个节点拆分为两个子节点。当子节点的误差与父节点的误差小于预先设定的阈值时，不再进行分支。</li><li>分类 or 回归</li></ul></li><li>Ensemble learning and Random forest<ul><li>Voting classifiers：由不同算法的模型组成，通过投票来进行决策</li><li>Bootstrap aggregating，Bagging：有相同的学习模型组成，通过 bootstrap 得到组合模型<ul><li>bootstrap 有助于防止过度拟合</li></ul></li><li>Random forest：对随机森林中的分类树进行分支时，遍历所有尚未进入分支的解释变量的一个随机子集</li></ul></li></ul></li><li><p>Unsupervised learning model</p><ul><li>降维：不影响数据解释能力的情况下降低数据维度<ul><li>PCA：把所有解释变量综合在一起进行正交分解，按照解释力度从高到低逐一分解</li></ul></li><li>聚类<ul><li>Hierarchical Clustering 分层聚类<ul><li>Divisive Clustering 分裂聚类：自上而下</li><li>Agglomerative Clustering 合并聚类：自下而上<ul><li>dendrograms 图</li></ul></li></ul></li><li>K-means ：自下而上<ol><li>随机产生 K 个 centroids 质心</li><li>计算每个数据点到 K 个质心的距离，将每个数据点归类为距离最近的质心所对应的类</li><li>将新的质心定义为归类到原质心的数据点的中点</li><li>如果新质心相比原质心变化很小，则停止算法；否则回到第二步</li></ol></li></ul></li></ul></li><li><p>Deep learning model</p><ul><li>Neural networks 神经网络<ul><li>如果 hidden layer 很多（如超过20层），那么也叫做 deep learning nets，DLNs</li></ul></li><li>Deep learning and reinforcement learning<ul><li>reinforcement learning 是指人工智能（agent）根据周围的环境采取行动，模型根据行动的结果给予人工智能奖励（reward）或惩罚，从而学习行动方式。e.g. AlphaGo</li></ul></li></ul></li></ol><p><img src=/imgs/img-lazy-loading.gif data-src=1%20Quantitative%20Methods%20e84dd0d2cb6140a5a200ff6cf9e94216/Untitled%202.png alt=Untitled></p><table><thead><tr><th>模型类别</th><th>分类</th><th>连续</th><th>分类或连续</th></tr></thead><tbody><tr><td>Supervised</td><td>Logistic<br>SVM<br>KNN<br>CART</td><td>Linear regression<br>Panelized regression<br>Logistic regression<br>CART<br>Random forest</td><td>Neural network</td></tr><tr><td>Deep learning<br>Reinforcement learning</td><td></td><td></td><td></td></tr><tr><td>Unsupervised</td><td>PCA<br>分层聚类<br>K-means</td><td>PCA<br>分层聚类<br>K-means</td><td>Neural network<br>Deep learning<br>Reinforcement learning</td></tr></tbody></table><h3 id=learning-module-7-big-data-projects>Learning Module 7: Big data projects
<a class=header-anchor href=#learning-module-7-big-data-projects></a></h3><ol><li>大数据特征：volume, variety, velocity, veracity（可靠性）</li><li>结构化数据与非结构化数据</li></ol><table><thead><tr><th></th><th>结构化数据</th><th>非结构化数据</th></tr></thead><tbody><tr><td>明确建模的目标</td><td>确认模型的输入和输出</td><td>文本分析(text problem formulation)，确认模型的输入和输出</td></tr><tr><td>数据收集</td><td></td><td>数据护理(data curation)</td></tr><tr><td>数据的准备与整理<br>1. data preparing/cleaning</td><td>- incompleteness error 数据不完整<br>- invalidity error 无效错误值<br>- inaccuracy error 数据不准确<br>- inconsistency error 数据不一致<br>- non-uniformity error 非标准错误<br>- duplication error 重复错误</td><td>- 删除 html 的标识符<br>- 删除断点符号 punctuations<br>- 删除数字<br>- 删除空白</td></tr><tr><td>2. data wrangling/data preprocessing</td><td>- extraction：从已有特征中构造新的变量<br>- aggregation：将两个或更多变量加总后得到类似的变量<br>- filtration：去掉不需要的行<br>- selection：去掉不需要的列<br>- conversion：将数据转换为合适的类型</td><td>- 将所有文本转化为小写<br>- 删除停止词 stop words，例如 the, is, a<br>- 词干提取 stemming<br>- 词形还原 lemmatization<br>—> bag-of-words</td></tr><tr><td></td><td>异常值 trimming or wisorization<br>normalization：<code>$\frac{X_i - X_{min}}{X_{max}-X_{min}}$</code><br>standardization：<code>$\frac{X_i-\mu}{\sigma}$</code></td><td></td></tr><tr><td>数据探索<br>1. 探索性数据分析 Exploratory data analysis, EDA</td><td>通过可视化图表发现数据关联</td><td>文本探索(text exploration)<br>- 统计单文本词频</td></tr><tr><td>2. 特征选择 Feature selection</td><td>- 反复迭代的过程<br>- 在提高模型解释力度和加快算法运行速度上进行抉择</td><td>精简文本标记符，噪声通常是出现频率最高或最低的词<br>- 利用频率删除噪声特征<br>- 卡方检验筛选特征<br>- 利用 mutual information 筛选特征</td></tr><tr><td>3. 特征工程 Feature Engineer</td><td>- 通过已有特征来构建新的特征</td><td>- 标记数字<br>- N-gram：词组<br>- 命名实体技术 name entity recognition, NER：识别专有名词<br>- 词性 parts of speech, POS</td></tr><tr><td>训练模型<br>1. 方法选择 method selection</td><td>- 监督模型与非监督模型的选择<br>- 数据的类型<br>- 数据的大小：观测值数据量较大时用神经网络模型，特征值较多时用支持向量机</td><td></td></tr><tr><td>2. 模型表现评估 performance evaluation</td><td>- 错误分析 Error analysis（下面表格）<br>- ROC， Receiver operating characteristic<br>- RMSE = <code>$\sqrt{\sum_{i=1}^n\frac{Predicted_i - Actual_i}{n}}$</code></td><td>同左</td></tr><tr><td>3. 模型调试 tuning</td><td>bias: 模型过于简单，欠拟合<br>variance：模型过于复杂，过度拟合</td><td></td></tr></tbody></table><ul><li>Error analysis<ul><li>confusion matrix</li></ul></li></ul><table><thead><tr><th>预测 vs 真实</th><th>1</th><th>0</th></tr></thead><tbody><tr><td>1</td><td>TP</td><td>FP ( Type I error)</td></tr><tr><td>0</td><td>FN (Type II error)</td><td>TN</td></tr></tbody></table><ul><li><p>Precision (P) = TP / ( TP + FP )</p><ul><li>模型预测为1的样本中，有多少真的是1</li></ul></li><li><p>Recall (R) = TP / ( TP + FN )</p><ul><li>真的是1的样本中，被预测为1的有多少</li></ul></li><li><p>Accuracy = ( TP + TN ) / ( TP + FP + TN + FN )</p></li><li><p>F1 score = 2<em>P</em>R/(P+R)</p><ul><li>P 和 R 的调和平均值</li><li>当数据分类分布不均匀时，F1比accuracy更适用，分数越高模型表现越好</li></ul></li><li><p>ROC</p><ul><li>False positive rate FPR = FP / ( TN + FP )</li><li>True positive rate TPR = TP / ( TP + FN ) = Recall</li><li>ROC 越往左上凸越好</li></ul><p><img src=/imgs/img-lazy-loading.gif data-src=1%20Quantitative%20Methods%20e84dd0d2cb6140a5a200ff6cf9e94216/Untitled%203.png alt=Untitled></p></li></ul></div><footer class=post-footer><div class=post-tags><a href=/tags/cfa>CFA</a></div><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/post/2024-04-15-cfa-level-ii-economics/ rel=next title="CFA Level II - Economics"><i class="fa fa-chevron-left"></i> CFA Level II - Economics</a></div><div class="post-nav-prev post-nav-item"><a href=/post/2024-04-14-2024-04-14/ rel=prev title=2024-04-14>2024-04-14
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class=footer><div class=footer-inner><div id=gtranslate class=google-translate><i class="fa fa-language"></i><div id=google_translate_element></div></div><div class=copyright>&copy;
<span itemprop=copyrightYear>2024
</span><span class=with-love><i class="fa fa-heart"></i>
</span><span class=author itemprop=copyrightHolder>J Song</span></div><div class=powered-by>由 <a href=https://gohugo.io title=0.123.0 target=_blank>Hugo</a> & <a href=https://github.com/hugo-next/hugo-theme-next title=4.5.3 target=_blank>Hugo NexT.Gemini</a> 强力驱动</div></div></footer><script type=text/javascript src=https://cdn.staticfile.org/animejs/3.2.1/anime.min.js defer></script><script type=text/javascript src=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.js defer></script><script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":true,"save":"manual"},"copybtn":true,"darkmode":false,"giscus":{"cfg":{"category":"Comments","categoryid":null,"emit":false,"inputposition":"top","mapping":"title","reactions":false,"repo":"username/repo-name","repoid":null,"theme":"preferred_color_scheme"},"js":"https://giscus.app/client.js"},"hostname":"//localhost:4321/","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"找到 ${hits} 个搜索结果","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"limit":1e3,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":true,"transition":{"collheader":"fadeInLeft","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"postmeta":{"comments":{"enable":true,"plugin":"waline"},"views":{"enable":false,"plugin":"busuanzi"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"vendor":{"plugins":"qiniu","router":"https://cdn.staticfile.org"},"version":"4.5.3","waline":{"cfg":{"emoji":false,"imguploader":false,"placeholder":"请文明发言哟 ヾ(≧▽≦*)o","reaction":true,"reactiontext":["点赞","踩一下","得意","不屑","尴尬","睡觉"],"reactiontitle":"你认为这篇文章怎么样？","requiredmeta":["nick","mail"],"serverurl":null,"sofa":"快来发表你的意见吧 (≧∀≦)ゞ","wordlimit":200},"css":{"alias":"waline","file":"dist/waline.css","name":"@waline/client","version":"2.13.0"},"js":{"alias":"waline","file":"dist/waline.js","name":"@waline/client","version":"2.13.0"}}}</script><script type=text/javascript src=/js/main.js defer></script><script type=text/javascript src=/js/math.js defer></script></body></html>